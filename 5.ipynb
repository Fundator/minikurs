{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning (DL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "\n",
    "_\"The universal approximation theorem of feed-forward neural networks states that every\n",
    "continuous function on a compact subset $R^n$ can be arbitrary well approximated by a\n",
    "feed-forward neural network with one hidden layer, a finite number of neurons, and some\n",
    "mild assumptions about the activation function\"_\n",
    "\n",
    "\n",
    "- What is a neuron?\n",
    "\n",
    "[<img src=\"resources/deep_learning/activition_function.png\" width=\"700\"/>](resources\\deep_learning\\activition_function.png)\n",
    "\n",
    "\n",
    "- What is a feed-forward neural network?\n",
    "\n",
    "[<img src=\"resources/deep_learning/neural_network_3.png\" width=\"500\"/>](resources/deep_learning/neural_network_3.png)\n",
    "\n",
    "\n",
    "\n",
    "- How can it fit a function at all?\n",
    "\n",
    "Simple approximation:\n",
    "[<img src=\"resources\\deep_learning\\simple_approx.png\" width=\"600\"/>](resources\\deep_learning\\simple_approx.png)\n",
    "\n",
    "Complex approximation:\n",
    "[<img src=\"resources\\deep_learning\\complex_approx.png\" width=\"600\"/>](resources\\deep_learning\\complex_approx.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network\n",
    "\n",
    "_\"The number of linear regions in a feed-forward neural network grows exponentially with depth $L$ and polynomial\n",
    "with the number of neurons $n$ per hidden layer. Therefore, the number of linear regions grows much faster for deep architectures compared to shallow architectures with $nL$\n",
    "hidden neurons\"_\n",
    "\n",
    "ResNet, a deep residual neural network:\n",
    "\n",
    "[<img src=\"resources/deep_learning/ResNet.png\" width=\"800\"/>](resources/deep_learning/ResNet.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it work?\n",
    "\n",
    "TODO: Describe feed-forward, backpropagation, and gradient decent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is it useful?\n",
    "\n",
    "Since can arbitrary well approximate any relationship between input and output, it is usefull for complex data structures such as natrual language, images, audio and signals, time-series, or combinations of different data structures. Applications are NLP, image analysis, audio and signal processing, time-series forcasting, and reinforcement learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Demonstations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No free lunch theorem\n",
    "\n",
    "There are no free lunch, and espesially not within machine learning! Establish a baseline with the simplest possible solution first, then try to beat it.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0a06b1e484fd1b448386af0311754ceeab002272e77b8d19b3ff8a0c76025355"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
